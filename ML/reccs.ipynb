{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07696c59",
   "metadata": {},
   "source": [
    "## Imporing all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1dda006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Ratings Matrix (R):\n",
      "        Item1  Item2  Item3  Item4\n",
      "User1    5.0    3.0    NaN      1\n",
      "User2    4.0    NaN    NaN      1\n",
      "User3    1.0    1.0    NaN      5\n",
      "User4    1.0    NaN    NaN      4\n",
      "User5    NaN    1.0    5.0      4\n",
      "Epoch 0, Loss: 10.6036\n",
      "Epoch 1000, Loss: 0.0000\n",
      "Epoch 2000, Loss: 0.0000\n",
      "Epoch 3000, Loss: 0.0000\n",
      "Epoch 4000, Loss: 0.0000\n",
      "\n",
      "ðŸ”® Predicted Ratings Matrix (R_hat):\n",
      "        Item1  Item2  Item3  Item4\n",
      "User1   5.00   3.00   0.28    1.0\n",
      "User2   4.00   2.42   0.48    1.0\n",
      "User3   1.00   1.00   6.34    5.0\n",
      "User4   1.00   0.92   5.03    4.0\n",
      "User5   1.14   1.00   5.00    4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------\n",
    "# 1. Mock User-Item Matrix\n",
    "# ------------------------\n",
    "R_df = pd.DataFrame([\n",
    "    [5, 3, np.nan, 1],\n",
    "    [4, np.nan, np.nan, 1],\n",
    "    [1, 1, np.nan, 5],\n",
    "    [1, np.nan, np.nan, 4],\n",
    "    [np.nan, 1, 5, 4]\n",
    "], columns=[\"Item1\", \"Item2\", \"Item3\", \"Item4\"], index=[\"User1\", \"User2\", \"User3\", \"User4\", \"User5\"])\n",
    "\n",
    "print(\"Original Ratings Matrix (R):\\n\", R_df)\n",
    "\n",
    "# Convert to numpy and mask missing values\n",
    "R = torch.tensor(R_df.fillna(0).values, dtype=torch.float32)\n",
    "mask = torch.tensor(~R_df.isna().values, dtype=torch.bool)\n",
    "\n",
    "num_users, num_items = R.shape\n",
    "k = 2 # number of latent features\n",
    "\n",
    "# ------------------------\n",
    "# 2. Define Model\n",
    "# ------------------------\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_users, num_items, k):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.user_factors = nn.Embedding(num_users, k)\n",
    "        self.item_factors = nn.Embedding(num_items, k)\n",
    "        nn.init.normal_(self.user_factors.weight, std=0.1)\n",
    "        nn.init.normal_(self.item_factors.weight, std=0.1)\n",
    "\n",
    "    def forward(self):\n",
    "        return torch.matmul(self.user_factors.weight, self.item_factors.weight.t())\n",
    "\n",
    "# ------------------------\n",
    "# 3. Train the Model\n",
    "# ------------------------\n",
    "model = MatrixFactorization(num_users, num_items, k)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    prediction = model()\n",
    "    loss = criterion(prediction[mask], R[mask])  # Only compute loss on known ratings\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ------------------------\n",
    "# 4. Get Predicted Ratings\n",
    "# ------------------------\n",
    "model.eval()\n",
    "R_hat = model().detach().numpy()\n",
    "R_hat_df = pd.DataFrame(R_hat, columns=R_df.columns, index=R_df.index)\n",
    "\n",
    "print(\"\\nðŸ”® Predicted Ratings Matrix (R_hat):\\n\", R_hat_df.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a058d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Ratings Matrix (R):\n",
      "        Item1  Item2  Item3  Item4\n",
      "User1    5.0    3.0    NaN      1\n",
      "User2    4.0    NaN    NaN      1\n",
      "User3    1.0    1.0    NaN      5\n",
      "User4    1.0    NaN    NaN      4\n",
      "User5    NaN    1.0    5.0      4\n",
      "Epoch 0, Loss: 137.8368\n",
      "Epoch 1000, Loss: 0.8258\n",
      "Epoch 2000, Loss: 0.8131\n",
      "Epoch 3000, Loss: 0.8046\n",
      "Epoch 4000, Loss: 0.7991\n",
      "\n",
      "ðŸ”® Predicted Ratings Matrix (R_hat):\n",
      "        Item1  Item2  Item3  Item4\n",
      "User1   4.95   2.97   3.12   1.00\n",
      "User2   3.96   2.39   2.72   1.00\n",
      "User3   1.00   0.99   5.99   4.94\n",
      "User4   1.00   0.90   4.88   3.97\n",
      "User5   1.17   1.01   4.97   3.98\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Mock user-item rating matrix (NaN = missing)\n",
    "R_df = pd.DataFrame([\n",
    "    [5, 3, np.nan, 1],\n",
    "    [4, np.nan, np.nan, 1],\n",
    "    [1, 1, np.nan, 5],\n",
    "    [1, np.nan, np.nan, 4],\n",
    "    [np.nan, 1, 5, 4]\n",
    "], columns=[\"Item1\", \"Item2\", \"Item3\", \"Item4\"], index=[\"User1\", \"User2\", \"User3\", \"User4\", \"User5\"])\n",
    "\n",
    "print(\"Original Ratings Matrix (R):\\n\", R_df)\n",
    "\n",
    "# Convert to NumPy array, mask missing values\n",
    "R = R_df.values\n",
    "mask = ~np.isnan(R)\n",
    "\n",
    "# Replace NaNs with zeros just for SGD (we'll use the mask to ignore them during training)\n",
    "R = np.nan_to_num(R)\n",
    "\n",
    "# Parameters\n",
    "num_users, num_items = R.shape\n",
    "k = 2  # number of latent features\n",
    "alpha = 0.01  # learning rate\n",
    "lambda_reg = 0.02  # regularization\n",
    "epochs = 5000\n",
    "\n",
    "# Initialize user (U) and item (V) latent matrices randomly\n",
    "U = np.random.normal(scale=0.1, size=(num_users, k))\n",
    "V = np.random.normal(scale=0.1, size=(num_items, k))\n",
    "\n",
    "# SGD\n",
    "for epoch in range(epochs):\n",
    "    for i in range(num_users):\n",
    "        for j in range(num_items):\n",
    "            if mask[i, j]:  # only update known ratings\n",
    "                eij = R[i, j] - np.dot(U[i, :], V[j, :])\n",
    "                # Update rules\n",
    "                U[i, :] += alpha * (eij * V[j, :] - lambda_reg * U[i, :])\n",
    "                V[j, :] += alpha * (eij * U[i, :] - lambda_reg * V[j, :])\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.sum((mask * (R - U @ V.T) ** 2)) + lambda_reg * (np.sum(U**2) + np.sum(V**2))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Final prediction\n",
    "R_pred = U @ V.T\n",
    "R_pred_df = pd.DataFrame(R_pred, columns=R_df.columns, index=R_df.index)\n",
    "\n",
    "print(\"\\nðŸ”® Predicted Ratings Matrix (R_hat):\\n\", R_pred_df.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d4250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  item_id                  name  similarity\n",
      "6    I007    White Tennis Shoes    0.426670\n",
      "1    I002         Blue Sneakers    0.150919\n",
      "7    I008          Green Hoodie    0.138134\n",
      "2    I003  Black Leather Jacket    0.024230\n",
      "5    I006       Fitness Tracker    0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load user interaction data and item metadata\n",
    "\n",
    "# Sample user interaction data (in reality, would come from CSV)\n",
    "user_df = pd.DataFrame([\n",
    "    {\n",
    "        \"user_id\": \"U001\",\n",
    "        \"wishlist_items\": '[\"I001\"]',\n",
    "        \"cart_items\": '',\n",
    "        \"browsed_items_history\": '[\"I004\", \"I005\"]',\n",
    "        \"ordered_history\": ''\n",
    "    }\n",
    "])\n",
    "# Sample item metadata\n",
    "item_df = pd.DataFrame([\n",
    "    {\"item_id\": \"I001\", \"name\": \"Red Running Shoes\", \"category\": \"Shoes\", \"brand\": \"Nike\", \"description\": \"Comfortable red running shoes for men\"},\n",
    "    {\"item_id\": \"I002\", \"name\": \"Blue Sneakers\", \"category\": \"Shoes\", \"brand\": \"Adidas\", \"description\": \"Casual blue sneakers for daily use\"},\n",
    "    {\"item_id\": \"I003\", \"name\": \"Black Leather Jacket\", \"category\": \"Clothing\", \"brand\": \"Zara\", \"description\": \"Stylish black leather jacket\"},\n",
    "    {\"item_id\": \"I004\", \"name\": \"Running Shorts\", \"category\": \"Clothing\", \"brand\": \"Puma\", \"description\": \"Lightweight shorts for runners\"},\n",
    "    {\"item_id\": \"I005\", \"name\": \"Sports Socks\", \"category\": \"Accessories\", \"brand\": \"Nike\", \"description\": \"High-quality cotton socks for sports\"},\n",
    "    {\"item_id\": \"I006\", \"name\": \"Fitness Tracker\", \"category\": \"Electronics\", \"brand\": \"Fitbit\", \"description\": \"Track your workouts and heart rate\"},\n",
    "    {\"item_id\": \"I007\", \"name\": \"White Tennis Shoes\", \"category\": \"Shoes\", \"brand\": \"Nike\", \"description\": \"Classic white tennis shoes\"},\n",
    "    {\"item_id\": \"I008\", \"name\": \"Green Hoodie\", \"category\": \"Clothing\", \"brand\": \"H&M\", \"description\": \"Comfortable green hoodie for cold weather\"},\n",
    "])\n",
    "\n",
    "# Prepare item features (text description)\n",
    "item_df = item_df.dropna(subset=[\"description\"])\n",
    "item_df[\"text\"] = (\n",
    "    item_df[\"category\"].astype(str) + \" \" +\n",
    "    item_df[\"brand\"].astype(str) + \" \" +\n",
    "    item_df[\"description\"].astype(str)\n",
    ")\n",
    "\n",
    "# TF-IDF vectorization of item text\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "item_tfidf = vectorizer.fit_transform(item_df[\"text\"])\n",
    "item_id_to_index = dict(zip(item_df[\"item_id\"], item_df.index))\n",
    "\n",
    "# Define weights for user interactions\n",
    "interaction_weights = {\n",
    "    \"ordered_history\": 3.0,\n",
    "    \"wishlist_items\": 2.0,\n",
    "    \"cart_items\": 1.5,\n",
    "    \"browsed_items_history\": 1.0\n",
    "}\n",
    "\n",
    "# Recommendation function using weighted content-based filtering\n",
    "def recommend_items_for_user(user_id, top_n=10):\n",
    "    try:\n",
    "        user_row = user_df[user_df['user_id'] == user_id].iloc[0]\n",
    "    except IndexError:\n",
    "        print(f\"User {user_id} not found.\")\n",
    "        return []\n",
    "\n",
    "    user_vector = None\n",
    "    total_weight = 0.0\n",
    "    interacted_items = set()\n",
    "\n",
    "    for interaction, weight in interaction_weights.items():\n",
    "        try:\n",
    "            items = json.loads(user_row[interaction])\n",
    "            indices = [item_id_to_index[i] for i in items if i in item_id_to_index]\n",
    "            interacted_items.update(items)\n",
    "\n",
    "            if indices:\n",
    "                vectors = item_tfidf[indices]\n",
    "                weighted_vector = vectors.mean(axis=0) * weight\n",
    "                user_vector = weighted_vector if user_vector is None else user_vector + weighted_vector\n",
    "                total_weight += weight\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if user_vector is None or total_weight == 0:\n",
    "        return []\n",
    "\n",
    "    # Normalize user vector\n",
    "    user_vector /= total_weight\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(np.asarray(user_vector), item_tfidf).flatten()\n",
    "    item_df['similarity'] = similarities\n",
    "\n",
    "    # Exclude already interacted items\n",
    "    recommendations = item_df[~item_df['item_id'].isin(interacted_items)]\n",
    "    recommendations = recommendations.sort_values(by='similarity', ascending=False)\n",
    "\n",
    "    return recommendations[['item_id', 'name', 'similarity']].head(top_n)\n",
    "\n",
    "# Example usage\n",
    "recommended = recommend_items_for_user(user_id=\"U001\", top_n=5)\n",
    "print(recommended)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff199e57",
   "metadata": {},
   "source": [
    "## Dynamic pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5983506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60024ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111b993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
